<?xml version="1.0" encoding="utf-8"?>
<!--<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V5.0//EN" "docbook.dtd">-->
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="WrappingMpiAndLegacyCode"><info><title xml:id="WrappingApplicationLegacycode_main">Wrapping Native MPI Application</title></info>

	<para>The <emphasis role="bold">Message Passing Interface (MPI)</emphasis>
	is a widely adopted communication library for parallel and distributed
	computing. This chapter explains how Proactive can help you to <emphasis role="bold">deploy</emphasis>, <emphasis role="bold">wrap</emphasis> and
	<emphasis role="bold">couple</emphasis> MPI applications for distributed
	environments</para>

	<note><para><emphasis role="bold">Please note that, for the moment, the wrapping of native MPI applications is only available for *nix Operating Systems</emphasis></para></note>

	<para>Three different MPI wrapping approaches are possible:</para>

	<itemizedlist>
		<listitem>
			<para><emphasis role="bold">Simple deployment of unmodified MPI applications (<xref linkend="Simple_Deploying_legacy"/>)</emphasis></para>
			<para>
				You will follow this approach if you want to take profit from ProActive GCM Deployment
				to handle deployment issues (access, allocation and remote execution)
				to deploy your existing MPI applications.
			</para>
		</listitem>

		<listitem>
			<para><emphasis role="bold">MPI code coupling (<xref linkend="MPI_code_coupling"/>)</emphasis></para>
			<para>
				This wrapping approach was designed to couple independent standalone MPI applications to make them interoperate, using a ProActive layer for inter-application communication.
			</para>
		</listitem>

		<listitem>
			<para><emphasis role="bold">MPI code wrapping (<xref linkend="MPI_code_wrapping"/>)</emphasis></para>
			<para>
				This wrapping approach consists on supporting "MPI to/from Java" communications which permit users to exchange data between MPI and Java applications.
			</para>
		</listitem>
	</itemizedlist>


	<para>The MPI Native Applications Wrapping is organized along three different packages:</para>

	<itemizedlist>
		<listitem>
			<para>
				<literal>org.objectweb.proactive.extensions.nativeinterface</literal> - contains the implementation of code wrapping (Java-C communication, data conversion and management)
			</para>
		</listitem>

		<listitem>
			<para>
				<literal>org.objectweb.proactive.extensions.nativeinterfacempi</literal> - contains the implementation of code coupling of MPI applications (including data conversion and MPI deployment management)
			</para>
		</listitem>

		<listitem>
			<para>
				<literal>org.objectweb.proactive.extensions.nativecode</literal> - contains a simple bootstrap class to launch wrapped distributed/parallel applications
			</para>
		</listitem>
	</itemizedlist>


	<section xml:id="Simple_Deploying_legacy"><info><title xml:id="Simple_Deploying_legacy_5">Simple Deployment of unmodified MPI applications</title></info>

		<para>
			As the name says, the simple deployment of MPI applications is intended to deploy standalone MPI applications.
			It does not require the usage of any specific API.
			It is just a matter of defining the application deployment in terms of GCMD and GCMA descriptors and loading/deploying these descriptors
			(please refer to <xref linkend="GCMDeployment"/> to get more information about GCM Deployment).
			The advantage of deploying an MPI application with GCM deployment is that you can benefit from de support of
			various network protocols and grid/cloud middlewares to transparently deploy
			your MPI applications without configuring the environment or using specific tools.
		</para>

		<para>
			The following descriptors are part of an application example, which can be deployed with the script <literal>HelloExecutableMPI.sh</literal>
			available in <literal>ProActive/examples/mpi/standalone_mpi</literal>.
		</para>

		<para>
			Initially, the GCMA application has to be defined. The following snippet of code shows the definition of an MPI application.
			In that case, it is a simple application that implements a parallel “hello world” in MPI:
		</para>

		<programlisting language="xml"><textobject><textdata  fileref="automatic_snippets/GCMA_MPI_Descriptor.snip"/></textobject></programlisting>

		<para>
			The definition of node providers follows the philosophy of a Java application deployment. But, instead of using an SSH (or any other) group, the group is defined as an MPI group.
			The following snippet of descriptor shows a simple GCMD which defines the deployment of the application over a set of distributed resources:
		</para>

		<programlisting language="xml"><textobject><textdata  fileref="automatic_snippets/GCMD_MPI_Descriptor.snip"/></textobject></programlisting>

		<para>Please not that, on the definition of the MPI group, some different tags can be used. The following tags are mandatory: </para>

		<itemizedlist>
			<listitem>
				<para>
					<emphasis role="bold">hostlist</emphasis> - list of resources where the MPI application will be deployed
				</para>
			</listitem>

			<listitem>
				<para>
					<emphasis role="bold">distributionPath</emphasis> - installation path of MPI. In the example shown above, MPI is supposed to be installed system-wide.
					So, the <literal>mpirun</literal> command can be found in <literal>/usr/bin</literal>
				</para>
			</listitem>

			<listitem>
				<para>
					<emphasis role="bold">commandPath</emphasis> - the deployment of MPI application by ProActive is actually done by scripts which are responsible for the configuration of the environment, before the execution.
					This is also the place where the runtime handles the different MPI distribution characteristics (e.g. LAM/MPI requires the creation of a virtual LAM environment).
				</para>

				<para>The ProActive middleware offers support for four different MPI distributions:</para>

				<itemizedlist>
					<listitem>
						<para>LAM/MPI: use the <literal>ProActive/scripts/gcmdeployment/executable_mpi_lam.sh</literal> commandPath
						</para>
					</listitem>

					<listitem>
						<para>MPICH or OpenMPI: use the <literal>ProActive/scripts/gcmdeployment/executable_mpi_mpich.sh</literal> commandPath
						</para>
					</listitem>

					<listitem>
						  <para>GridMPI: use the <literal>ProActive/scripts/gcmdeployment/ProActive/scripts/gcmdeployment/executable_gridmpi.sh</literal> commandPath
						  </para>
					</listitem>
				</itemizedlist>

				<para>But you can use any other distribution by defining a similar scripts and using them on GCMD.</para>
			</listitem>

		</itemizedlist>

		<para>The following tags are optional on the definition of <emphasis role="bold">MPIGroups</emphasis>:</para>

		<itemizedlist>
			<listitem>
				<para>
					<emphasis role="bold">commandOptions</emphasis> - list of MPI-related option to be appended on the <literal>mpirun </literal> command at deployment time
				</para>
			</listitem>

			<listitem>
				<para>
					<emphasis role="bold">machineFile</emphasis> - file containing a list of machines. If this file is not provided, the runtime automatically creates the <literal>machineFile</literal>,
					including the hostnames defined in the <literal>hostlist</literal> tag. If the file is generated at runtime, it is deleted prior to the end of the execution.
				</para>
			</listitem>

			<listitem>
				<para>
					<emphasis role="bold">execDir</emphasis> - directory where the <literal>mpirun</literal> command will be executed.
					This allows the use of relative path on the definition of the MPI binary on the GCMA descriptor.
				</para>
			</listitem>

		</itemizedlist>


		<para>Once you have defined the 2 descriptors (GCMA and GCMD), the deployment of the application is pretty straightforward:</para>

		<programlisting language="java"><textobject><textdata  fileref="automatic_snippets/Hello_MPI_example.snip"/></textobject></programlisting>

	</section>

	<section xml:id="MPI_code_coupling"><info><title xml:id="MPI_code_coupling_5"> MPI Code Coupling</title></info>
		<para>
			This wrapping approach was designed to couple independent standalone MPI applications to make them interoperate,  using a dedicated MPI-like communication API and seamless using a ProActive communication inter-application layer.
			Besides the ease of deployment, the MPI code coupling can be used to perform transparent inter-cluster communication, even if nodes do not present a direct link among them.
		</para>
		<para>There are two basic usages for the ProActive/MPI coupling approach: </para>

		<itemizedlist>
			<listitem>
				<para>
					<emphasis role="bold">Single MPI application</emphasis> - the idea of code coupling in the context of a single application is to make possible the execution
					of a single MPI application across a multi-domain environment, which could be composed of clusters, grids and/or cloud resources.
					This usage is specially intended for embarrassingly distributed/parallel applications or applications parallelized upon a domain-decomposition approach.
					For instance, you might have a large data mesh and intend to distribute more this mesh to avoid memory swapping and cash misses.
				</para>
			</listitem>

			<listitem>
				<para>
					<emphasis role="bold">Multiple MPI applications</emphasis>: this is a more loosely coupled approach which allows to loose-coupling multiple MPI applications
					which can be deployed in different clusters, grids and/or cloud resources.
					This usage is specially intended for distributed and parallel applications parallelized upon a functional-decomposition approach.
					For instance, in a typical weather modeling, you might have multiple independent numerical kernels which implement different models
					(hydrology, pressure and temperature, for instance), processing independent sets of data and explicit data dependencies among these kernels.
				</para>
			</listitem>
		</itemizedlist>

		<para> In both cases, the MPI-coupling scenario looks like the following picture:</para>
		<para>
			<figure xml:id="MPI_Coupling2"><info><title>MPI-coupling scenario</title></info>
				<mediaobject>
					<imageobject>
						<imagedata scalefit="1" width="100%" contentdepth="100%" fileref="images/png/mpi-coupling.png" format="PNG"/>
					</imageobject>
				</mediaobject>
			</figure>
		</para>

		<para>
			On this picture, we can identify two independent MPI applications (Application 0 and Application 1) running on two sets of resources.
			Internally, each of these applications communicates through standard MPI and the two applications can communicate through the ProActive/MPI communication layer,
			due to an API defined on <xref linkend="MPI_code_coupling_API"/>.
		</para>
		<para>
			Each of the processes receives a hierarchical identification <literal>X:Y</literal>, being <literal>X</literal> the application ID and <literal>Y</literal> the MPI rank.
			By using the API and the hierarchical ranks, the different applications can address process of different applications to communicate.
		</para>

		<section xml:id="MPI_code_coupling_API"><info><title xml:id="MPI_code_coupling_API_5">MPI Code Coupling API</title></info>

			<para>
				The MPI Code Coupling API is a simplified MPI-like set of primitives, exposed through C/C++ and Fortran bindings.
				This API is intended to enable the explicit communication among the different MPI applications.
				Since ProActive plays just a middleware role in the case of code coupling, there is no Java API associated to code coupling.
				<xref linkend="MPI_code_wrapping"/> presents an API that allows Native-Java communication.
			</para>


			<!-- ///// ProActive API Definition \\\\ -->
			<programlisting language="c"><textobject><textdata  fileref="automatic_snippets/ProActiveMPI.snip"/></textobject></programlisting>

		</section>

		<section xml:id="MPI_code_coupling_Ex"><info><title xml:id="MPI_code_coupling_ex_5"> MPI Code Coupling Deployment and Example</title></info>

			<para>
				This section presents an example of code coupling available in <literal>ProActive/src/Examples/org/objectweb/proactive/examples/mpi/proactive_mpi/</literal>.
				The following code is the MPI implementation of a small benchmark of the point-to-point synchronous and asynchronous communication between processes in two different MPI applications:
			</para>

			<programlisting language="c"><textobject><textdata fileref="automatic_snippets/MPI_Coupling_Example.snip"/></textobject></programlisting>

			<para>
				In general, the structure of a ProActive-coupled MPI application application looks like a standard MPI application:
				it starts by an initial handshake to start the ProActive/MPI application (ProActiveMPI_Init) and obtain identifiers (MPI_Comm_rank and ProActiveMPI_Job).
				Then, it follows by the application core. This application core may contain the numerical kernels which involve communication and thus the use of MPI and ProActiveMPI applications.
				Eventually, it ends with the finalization of the application (ProActiveMPI_Finalize).
			</para>

			<note>
				<para>
					ProActive/MPI applications have a communication semantic similar to MPI applications.
					This means that, despite the object-oriented implementation of the communication layer, communications are, in general, two-side operations (send/receive).
					The transport implementation in the context of the ProActive layer is completely asynchronous, but there is no relation between the asynchronous calls with futures
					and the Native-side of applications. For this reason, you should, as in MPI, prefer asynchronous communications over synchronous ones to improve applications performance.
				</para>
			</note>

			<para>
				Since this is a ProActive example, it can be simply compiled within the Ant target <literal>compile.extensions</literal>.
				Yet, you may need to compile/link your applications with ProActiveMPI native libraries in order to deploy your own applications.
				In order to do that, you will require the following steps:
			</para>

			<orderedlist>
				<listitem>
					<para>First you must be under a *nix system and have MPI installed</para>
				</listitem>
				<listitem>
					<para>Compile the <literal>compile.extensions</literal> Ant target to create native libraries</para>
				</listitem>
				<listitem>
					<para>Include the folder <literal>ProActive/dist/lib/native</literal> (<literal>-L</literal>ProActive/dist/lib/native)</para>
				</listitem>
				<listitem>
					<para>
						Include the folders <literal>ProActive/classes/Extensions/org/objectweb/proactive/extensions/nativeinterface/</literal> and
						<literal>ProActive/classes/Extensions/org/objectweb/proactive/extensions/nativeinterfacempi/control/config/src/</literal> to the compilation
						(<literal>gcc / mpicc -I</literal>)
					</para>
				</listitem>
				<listitem>
					<para>
						Include the libraries <literal>rt</literal>, <literal>lProActiveNativeInterfaceIPC</literal> and <literal>ProActiveMPIComm </literal>
						(<literal> -lProActiveNativeInterfaceIPC -lProActiveMPIComm </literal>).
					</para>
			   </listitem>
			</orderedlist>

			<para>Thus, your compilation command will look a bit like: </para>

			<screen>mpicc -O3 -lrt -I${ProActive}/classes/Extensions/org/objectweb/proactive/extensions/nativeinterfacempi/control/config/src/
	-I${ProActive}classes/Extensions/org/objectweb/proactive/extensions/nativeinterface/
	-L${ProActive}/dist/lib/native
	-lProActiveNativeInterfaceIPC
	-lProActiveMPIComm your_application.c -o your_application</screen>

			<para>
				Once you have your application compiled, you will have to write some deployment descriptors
				to deploy the java communication layer and each of the independent MPI applications.
			</para>

			<para>
				The Java descriptor looks exactly like the descriptors presented in <xref linkend="GCMDeployment"/>.
				The only extra requirement is the definition of a <literal>-Djava.library.path</literal>
				option which is the place where the Java runtime will load the native library that allows Java-C communication.
				This options has to point to <literal>ProActive/dist/lib/native</literal>.
			</para>

			<para>ProActiveMPI GCMA:</para>

			<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_PA_GCMA.snip"/></textobject></programlisting>

			<para>In the GCMA definition, you has to define as many resource providers as independent MPI applications.</para>
			<note><para>Pay attention that the resources used for the runtime deployment has to be the same used for MPI execution.</para></note>

			<para>ProActiveMPI GCMD:</para>

			<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_PA_GCMD.snip"/></textobject></programlisting>

			<para>Note that we have defined the deployment of one runtime by JVM since the ProActiveMPI runtime are singleton and then, it is important to avoid runtime co-allocation.</para>

			<para>The MPI deployment descriptors are exactly like the ones presented in <xref linkend="Simple_Deploying_legacy"/>.</para>

			<para>MPI GCMA:</para>

			<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_MPI_GCMA.snip"/></textobject></programlisting>

			<para>MPI GCMD:</para>

			<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_MPI_GCMD.snip"/></textobject></programlisting>

			<para>
				Once you have defined these descriptors, you can use the <literal>org.objectweb.proactive.extensions.nativecode.NativeStarter </literal> class.
				This class receives as parameters the Java ProActive GCMA and the MPI GCMAs:
			</para>

			<screen>org.objectweb.proactive.extensions.nativecode.NativeStarter gcma.pa.xml gcma.mpi1.xml gcma.mpi2.xml ... </screen>
		</section>

		<section xml:id="DiscoGrid"><info><title xml:id="DiscoGrid_5">The DiscoGrid Project</title></info>

			<para>
				The <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www-sop.inria.fr/nachos/team_members/Stephane.Lanteri/DiscoGrid/">DiscoGrid</link> project aims
				at studying and promoting a new paradigm for programming non-embarrassingly parallel scientific computing applications on distributed, heterogeneous, computing platforms.
				The target applications require the numerical resolution of systems of partial differential equations (PDEs) modeling electromagnetic wave propagation and fluid flow problems.
			</para>
			<para>
				In the context of the DiscoGrid project, the ProActive team developed a GCM component-based runtime capable of coupling and deploying MPI applications over heterogeneous
				multi-domain infrastructures, composed by clusters, grids and clouds with the seamless treatment of complex network configurations (including firewalls and NAT).
			</para>

			<para>
				If you are interested in more advanced use of the ProActive/MPI code coupling approach, you will probably be interested in the DiscoGrid project.
				Please refer to the <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://gforge.inria.fr/projects/dg-proactive/">the DiscoGrid forge</link>
				for documentation and download of the DiscoGrid distribution.
			</para>
		</section>
	</section> <!--closes MPI Code Coupling-->

	<section xml:id="MPI_code_wrapping"><info><title xml:id="MPI_code_wrapping_5">MPI Code Wrapping</title></info>

		<para>
			This wrapping approach was designed to couple native applications and Java codes so as to make them interoperate by the usage of a simplified message passing-based API.
			This approach allows the development of native applications or numerical kernels to be bind to a native implementation.
		</para>

		<para>
			Two scenarios motivate this wrapping approach:
		</para>

		<itemizedlist>
			<listitem>
				<para>
					<emphasis role="bold">Use of existing numerical kernels or applications</emphasis>:
					Let us assume that you want to have part of your Java application to be handled by an existing native code
					to improve performance and/or to simply reuse these native kernels instead of translating them into Java.
					This is particularly useful for highly optimized native applications which can not be easily ported to Java.
				</para>
			</listitem>

			<listitem>
				<para>
					<emphasis role="bold"> Use of resources not available in Java (libraries, CUDA, etc.)</emphasis>:
					unfortunately, a number of libraries are just available in other programming languages (such as C/C++ and Fortran).
					This is the case of numerous numerical libraries and libraries to handle hardware devices (for instance, CUDA computing engine).
				</para>
			</listitem>
		</itemizedlist>

		<para>
			In both cases, all you need is to perform a deployment of the native application (<xref linkend="Executable_64"/>) and
			of the Java application (<xref linkend="GCMDeployment"/>), and use an specific API to make them communicate.
			The compilation of the native application and its deployment follow exactly the same approach presented in <xref linkend="MPI_code_coupling"/>.
			The main difference relies on the use of your own Java application instead of the <literal>NativeStarter</literal>.
		</para>

		<section xml:id="Code_wrapping_API"><info><title xml:id="Code_wrapping_API_5"> Code Wrapping API</title></info>

			<para>
				The MPI Code Wrapping API is a simplified message passing based set of primitives, exposed on the native side through C/C++ bindings (<xref linkend="MPI_code_coupling_API"/>),
				and with a couple of API classes on the Java side.
			</para>

			<section xml:id="Native_code_wrapping_API"><info><title xml:id="Native_code_wrapping_API_5">Native Code Wrapping API</title></info>

				<para>These are the main primitives available to init/finalize the binding of native and Java applications and communicate native-java processes:</para>

				<programlisting language="c"><textobject><textdata fileref="automatic_snippets/native_layer.snip"/></textobject></programlisting>

			</section>  <!--Closes Native API-->


			<section xml:id="Java_code_wrapping_API"><info><title xml:id="Java_code_wrapping_API_5">Java Code Wrapping API</title></info>

				<para>The Java API is mainly offered through 2 classes: </para>

				<itemizedlist>
					<listitem>
						<para>Communication and Code Coupling, on the <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="../api_complete/org/objectweb/proactive/extensions/nativeinterface/coupling/NativeInterface.html"> <literal>org.objectweb.proactive.extensions.nativeinterface.coupling.NativeInterface</literal></link> class.</para>
					</listitem>
					<listitem>
						<para>Data conversion, on the <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="../api_complete/org/objectweb/proactive/extensions/nativeinterface/utils/ProActiveNativeUtil.html"> <literal>org.objectweb.proactive.extensions.nativeinterface.utils.ProActiveNativeUtil</literal></link> class.</para>
					</listitem>
				</itemizedlist>

				<para>
					If you want a practical use of these APIs, please check <xref linkend="MPI_code_coupling"/> and the Java and C implementations
					of the MPI Code coupling in the ProActive package <literal>org.objectweb.proactive.extensions.nativeinterfacempi</literal>.
				</para>

			</section> <!--Closes Java API-->

		</section> <!--Closes API-->

	</section>

</chapter>
